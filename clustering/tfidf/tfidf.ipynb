{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', 'notebook_format'))\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-11-02 15:18:54 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.13.3\n",
      "pandas 0.20.3\n",
      "matplotlib 2.1.0\n",
      "sklearn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors\n",
    "\n",
    "When exploring a large set of text documents -- such as Wikipedia, news articles, StackOverflow, etc. -- it can be useful to get a list of related material. To find relevant documents you typically need to convert the text document into bag of words or TF-IDF format and choose the distance metric to measure the notion of simiarity. In this documentation we'll explore the tradeoffs with representing documents using bag of words and TF-IDF.\n",
    "\n",
    "We will be using the Wikipedia pages dataset. Each row of the dataset consists of a link to the wikipedia article, the name of the person, and the text of the article (in lowercase). To follow along, please download the file from this [dropbox link](https://www.dropbox.com/s/mriz0nq35ore8cr/people_wiki.csv?dl=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = os.path.join('..', 'people_wiki.csv')\n",
    "wiki = pd.read_csv(filepath)\n",
    "wiki.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by converting the document into bag of words format and utilize the euclidean distance to find the nearest neighbors of the Barack Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35817</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24478</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>33.015148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28447</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>34.307434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14754</td>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>35.791060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35357</td>\n",
       "      <td>Lawrence Summers</td>\n",
       "      <td>36.069378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31423</td>\n",
       "      <td>Walter Mondale</td>\n",
       "      <td>36.249138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13229</td>\n",
       "      <td>Francisco Barrio</td>\n",
       "      <td>36.276714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36364</td>\n",
       "      <td>Don Bonker</td>\n",
       "      <td>36.400549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22745</td>\n",
       "      <td>Wynn Normington Hugh-Jones</td>\n",
       "      <td>36.441734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7660</td>\n",
       "      <td>Refael (Rafi) Benvenisti</td>\n",
       "      <td>36.837481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        name   distance\n",
       "0  35817                Barack Obama   0.000000\n",
       "1  24478                   Joe Biden  33.015148\n",
       "2  28447              George W. Bush  34.307434\n",
       "3  14754                 Mitt Romney  35.791060\n",
       "4  35357            Lawrence Summers  36.069378\n",
       "5  31423              Walter Mondale  36.249138\n",
       "6  13229            Francisco Barrio  36.276714\n",
       "7  36364                  Don Bonker  36.400549\n",
       "8  22745  Wynn Normington Hugh-Jones  36.441734\n",
       "9   7660    Refael (Rafi) Benvenisti  36.837481"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of words\n",
    "vect = CountVectorizer()\n",
    "word_weight = vect.fit_transform(wiki['text'])\n",
    "\n",
    "# query Barack Obama's top 10 nearest neighbors\n",
    "nn = NearestNeighbors(metric = 'euclidean')\n",
    "nn.fit(word_weight)\n",
    "obama_index = wiki[wiki['name'] == 'Barack Obama'].index[0]\n",
    "distances, indices = nn.kneighbors(word_weight[obama_index], n_neighbors = 10)\n",
    "\n",
    "# 1. flatten the 2d-array distance and indices to 1d\n",
    "# 2. merge the distance information with the original wiki dataset, \n",
    "#    to obtain the name matching the id.\n",
    "neighbors = pd.DataFrame({'distance': distances.flatten(), 'id': indices.flatten()})\n",
    "nearest_info = (wiki.\n",
    "                merge(neighbors, right_on = 'id', left_index = True).\n",
    "                sort_values('distance')[['id', 'name', 'distance']])\n",
    "nearest_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result, it seems nice that all of the 10 people are politicians. Let's dig a bit deeper and find out why Joe Biden was considered a close neighbor of Obama by looking at the most frequently used words in both pages. To do this, we'll take the sparse row matrix that we had and extract the word count dictionary for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_word_weight(vect, word_weight):\n",
    "    \"\"\"\n",
    "    Given the CountVector and the fit_transformed\n",
    "    word count sparse array obtain each documents' \n",
    "    word count dictionary\n",
    "    \n",
    "    In the Compressed Sparse Row format,\n",
    "    `indices` stands for indexes inside the row vectors of the matrix \n",
    "    and `indptr` (index pointer) tells where the row starts in the data and in the indices \n",
    "    attributes. nonzero values of the i-th row are data[indptr[i]:indptr[i+1]] \n",
    "    with column indices indices[indptr[i]:indptr[i+1]]\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    http://www.scipy-lectures.org/advanced/scipy_sparse/csr_matrix.html\n",
    "    \"\"\"\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "    data = word_weight.data\n",
    "    indptr = word_weight.indptr\n",
    "    indices = word_weight.indices\n",
    "    n_docs = word_weight.shape[0]\n",
    "    \n",
    "    word_weight_list = []\n",
    "    for i in range(n_docs):\n",
    "        doc = slice(indptr[i], indptr[i + 1])\n",
    "        count, idx = data[doc], indices[doc]\n",
    "        feature = feature_names[idx]\n",
    "        word_weight_dict = Counter({k: v for k, v in zip(feature, count)})\n",
    "        word_weight_list.append(word_weight_dict)\n",
    "    \n",
    "    return word_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>word_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "      <td>{'drawn': 1, 'delisted': 1, 'until': 1, 'excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "      <td>{'circadian': 3, 'sack': 1, 'bright': 1, 'deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "      <td>{'band': 3, 'association': 1, 'crossed': 1, 'r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            URI            name  \\\n",
       "0   <http://dbpedia.org/resource/Digby_Morrell>   Digby Morrell   \n",
       "1  <http://dbpedia.org/resource/Alfred_J._Lewy>  Alfred J. Lewy   \n",
       "2   <http://dbpedia.org/resource/Harpdog_Brown>   Harpdog Brown   \n",
       "\n",
       "                                                text  \\\n",
       "0  digby morrell born 10 october 1979 is a former...   \n",
       "1  alfred j lewy aka sandy lewy graduated from un...   \n",
       "2  harpdog brown is a singer and harmonica player...   \n",
       "\n",
       "                                         word_weight  \n",
       "0  {'drawn': 1, 'delisted': 1, 'until': 1, 'excha...  \n",
       "1  {'circadian': 3, 'sack': 1, 'bright': 1, 'deve...  \n",
       "2  {'band': 3, 'association': 1, 'crossed': 1, 'r...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki['word_weight'] = unpack_word_weight(vect, word_weight)\n",
    "wiki.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(wiki, name, column_name, top_n = None):\n",
    "    row = wiki.loc[wiki['name'] == name, column_name]\n",
    "\n",
    "    # when converting Series to dictionary, the row index \n",
    "    # will be the key to the content\n",
    "    word_weight_dict = row.to_dict()[row.index[0]]\n",
    "    \n",
    "    if top_n is None:\n",
    "        top_n = len(word_weight_dict)\n",
    "    \n",
    "    word_weight_table = pd.DataFrame(word_weight_dict.most_common(top_n), \n",
    "                                     columns = ['word', 'weight'])\n",
    "    return word_weight_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Biden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>his</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  Obama  Biden\n",
       "0  the     40     33\n",
       "1   in     30     16\n",
       "2  and     21     19\n",
       "3   of     18     12\n",
       "4   to     14     11\n",
       "5  his     11      5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_obama = get_top_words(wiki, name = 'Barack Obama', column_name = 'word_weight')\n",
    "words_biden = get_top_words(wiki, name = 'Joe Biden', column_name = 'word_weight')\n",
    "\n",
    "# merge the two DataFrame, since both tables contained the same column name weight, \n",
    "# it will automatically renamed one of them by adding suffix _x and _y to prevent confusion.\n",
    "# hence we'll rename the columns to tell which one is for which\n",
    "words_combined = (words_obama.\n",
    "                  merge(words_biden, on = 'word').\n",
    "                  rename(columns = {'weight_x': 'Obama', 'weight_y': 'Biden'}))\n",
    "words_combined.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question we want to ask is, among these common words that appear in both documents, how many other documents in the Wikipedia dataset also contain them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_top_words(word_weight_vector, common_words):\n",
    "    \"\"\"\n",
    "    return True if common_words is a subset of unique_words\n",
    "    return False otherwise\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    http://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series\n",
    "    \"\"\"\n",
    "    # extract the keys of word_weight_vector and convert it to a set\n",
    "    unique_words = set(word_weight_vector.keys())\n",
    "    boolean = common_words.issubset(unique_words)\n",
    "    return boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 common words:  {'in', 'of', 'to', 'the', 'and'}\n",
      "number of articles that also contain the common words:  56066\n"
     ]
    }
   ],
   "source": [
    "# we'll extract the 5 most frequent\n",
    "common_words = set(words_combined['word'].head(5))\n",
    "print('top 5 common words: ', common_words)\n",
    "\n",
    "wiki['has_top_words'] = wiki['word_weight'].apply(has_top_words, args = (common_words,))\n",
    "print('number of articles that also contain the common words: ', wiki['has_top_words'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give this result, we saw that much of the perceived commonalities between the two articles were due to occurrences of extremely frequent words, such as \"the\", \"and\", and \"his\". All of these words appear very often in all of the documents. To retrieve articles that are more relevant, maybe we should be focusing more on rare words that don't happen in every article. And this is where TF-IDF (term frequency–inverse document frequency) comes in. Note that although we can remove stop words to prevent some of this behavior, sometimes the frequently appeared words might not be a stop word.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "TF-IDF, short for term frequency–inverse document frequency, is a numeric measure that is use to score the importance of a word in a document based on how often did it appear in that document and a given collection of documents. The intuition for this measure is : If a word appears frequently in a document, then it should be important and we should give that word a high score. But if a word appears in too many other documents, it’s probably not a unique identifier, therefore we should assign a lower score to that word. In short, it is a feature representation that penalizes words that are too common.\n",
    "\n",
    "Let us consider the following toy dataset that consists of 3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly, TF-IDF is calculated as follows:\n",
    "\n",
    "$$\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, d, D)$$\n",
    "\n",
    "Where `t` denotes the terms; `d` denotes each document; `D` denotes the collection of documents.\n",
    "\n",
    "The first part of the formula $tf(t, d)$ stands for term frequency, which is defined by the number of times a term $t$ occurs in a document $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tf = vect.fit_transform(docs).toarray()\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'is': 1, 'shining': 2, 'sun': 3, 'sweet': 4, 'the': 5, 'weather': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the vocabulary, the word \"and\" would be the first column in each document vector in `tf` and it appears once in the third document.\n",
    "\n",
    "In order to understand the second part of the formlua $\\text{idf}(t, d, D)$, inverse document frequency, let's first write down the complete math formula for IDF.\n",
    "\n",
    "$$ idf(t, d, D) = log \\frac{ \\mid \\text{ } D \\text{ } \\mid }{ 1 + \\mid \\{ d : t \\in d \\} \\mid } $$\n",
    "\n",
    "- The numerator : `D` is infering to our document space. It can also be seen as D = ${ d_{1}, d_{2}, \\dots, d_{n} }$ where n is the number of documents in your collection. Thus for our example $\\mid \\text{ } D \\text{ } \\mid$, the size of our document space is 3, since we're have 3 documents\n",
    "- The denominator : $\\mid \\{ d: t \\in d \\} \\mid$ is the document freqency. To be explicit, it is the number of documents $d$ that contain the term $t$. Note that this implies it doesn't matter if a term appeared 1 time or 100 times in a document, it will still be counted as 1, since it simply did appear in the document\n",
    "- The constant 1 is added to the denominator to avoid a zero-division error if a term is not contained in any document in the test dataset\n",
    "\n",
    "Note that there are very different variation of the formula. For example, The tf-idfs in scikit-learn are calculated by:\n",
    "\n",
    "$$ idf(t, d, D) = log \\frac{ \\mid \\text{ } D \\text{ } \\mid }{ \\mid \\{ d : t \\in d \\} \\mid } + 1 $$\n",
    "\n",
    "Here, the `+1` count is added directly to the idf, instead of the denominator. The effect of this is that terms with zero idf, i.e. that occur in all documents of a training set, will not be entirely ignored. We can demonstrate this by calculating the idfs manually using the equation above and do the calculation ourselves and compare the results to the TfidfTransformer output using the settings `use_idf=True, smooth_idf=False, norm=None` (more on `smooth_idf` and `norm` later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          1.          1.40546511  1.40546511  0.          1.          0.        ]\n",
      "\n",
      "[ 0.          1.          1.40546511  1.40546511  0.          1.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# compute manually the tfidf score for the first document\n",
    "n_docs = len(docs)\n",
    "df = np.sum(tf != 0, axis = 0)\n",
    "idf = np.log(n_docs / df) + 1\n",
    "tf_idf = tf[0] * idf\n",
    "print(tf_idf)\n",
    "print()\n",
    "\n",
    "# use the library to do the computation\n",
    "tfidf = TfidfTransformer(use_idf = True, smooth_idf = False, norm = None)\n",
    "doc_tfidf = tfidf.fit_transform(tf).toarray()\n",
    "print(doc_tfidf[0])\n",
    "\n",
    "assert np.allclose(tf_idf, doc_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, recall that in the tf (term frequency) section, we’re representing each term as the number of times they appeared in the document. The main issue for this representation is that it will create a bias towards long documents, as a given term has more chance to appear in longer document, making them look more important than actually they are. Thus the approach to resolve this issue is the good old L2 normalization. i.e., dividing the raw term frequency vector $v$ by its length $||v||_2$ (L2- or Euclidean norm).\n",
    "\n",
    "$$v_{norm} = \\frac{v}{ \\parallel v \\parallel_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}} = \\frac{v}{\\big(\\sum_{i=1}^n v_i^2 \\big)^{\\frac{1}{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.40993715  0.57615236  0.57615236  0.          0.40993715\n",
      "  0.        ]\n",
      "\n",
      "[ 0.          0.40993715  0.57615236  0.57615236  0.          0.40993715\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# manual\n",
    "tf_norm = tf_idf / np.sqrt(np.sum(tf_idf ** 2))\n",
    "print(tf_norm)\n",
    "print()\n",
    "\n",
    "# library\n",
    "tfidf = TfidfTransformer(use_idf = True, smooth_idf = False, norm = 'l2')\n",
    "doc_tfidf = tfidf.fit_transform(tf).toarray()\n",
    "print(doc_tfidf[0])\n",
    "\n",
    "assert np.allclose(tf_norm, doc_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter in the `TfidfTransformer` is the `smooth_idf`, which is described as\n",
    "\n",
    "> smooth_idf : boolean, default=True  \n",
    "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "\n",
    "So, our idf would then be defined as follows:\n",
    "\n",
    "$$idf(t, d, D) = log \\frac{ 1 + \\mid D \\text{ } \\mid }{ 1 + \\mid \\{ d : t \\in d \\} \\mid } + 1 $$ \n",
    "\n",
    "Again let's confirm this by computing it manually and using the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.43370786  0.55847784  0.55847784  0.          0.43370786\n",
      "  0.        ]\n",
      "\n",
      "[ 0.          0.43370786  0.55847784  0.55847784  0.          0.43370786\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# manual\n",
    "n_docs = len(docs)\n",
    "df = np.sum(tf != 0, axis = 0)\n",
    "idf = np.log((1 + n_docs) / (1 + df)) + 1\n",
    "tf_idf = tf[0] * idf\n",
    "tf_norm = tf_idf / np.sqrt(np.sum(tf_idf ** 2))\n",
    "print(tf_norm)\n",
    "print()\n",
    "\n",
    "# library\n",
    "tfidf = TfidfTransformer(use_idf = True, smooth_idf = True, norm = 'l2')\n",
    "doc_tfidf = tfidf.fit_transform(tf).toarray()\n",
    "print(doc_tfidf[0])\n",
    "\n",
    "assert np.allclose(tf_norm, doc_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, the default settings in the `TfidfTransformer` is:\n",
    "\n",
    "- `use_idf=True`\n",
    "- `smooth_idf=True`\n",
    "- `norm='l2'`\n",
    "\n",
    "And we can use the `TfidfVectorizer` to compute the TF-IDF score from raw text in one step without having to do use `CountVectorizer` to convert it to bag of words representation and then transform it to TF-IDF using `TfidfTransformer`. For those interested, this [link](https://github.com/ethen8181/machine-learning/blob/master/text_classification/tfidf/feature_extraction.py) contains the full TF-IDF implemented from scratch.\n",
    "\n",
    "Now that we've understood the inner details on TF-IDF let's return back to our initial task and use this weighting scheme instead of bag of words. We start by converting the document into TF-IDF format and use this along with cosine distance to find the nearest neighbors of the Barack Obama (if we normalized our articles in the TF-IDF transformation, then the euclidean distance and the cosine distance is proportional to each other, hence they're doing the same thing). Then we extract the commonly used words (now weighted by the TF-IDF score instead of word count) in both the Joe Biden and Obama article. Finally we compute how many other documents in the Wikipedia dataset also contain these words. In short, we're doing the exact same thing as the beginning except we now use TF-IDF to represent the documents and use cosine distance to measure similarity between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf instead of bag of words\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_weight = tfidf_vect.fit_transform(wiki['text'])\n",
    "\n",
    "# this time, query Barack Obama's top 100 nearest neighbors using\n",
    "# cosine distance, we have to specify the algorithm to `brute` since\n",
    "# the default 'auto' does not work with cosine distance\n",
    "# http://stackoverflow.com/questions/32745541/dbscan-error-with-cosine-metric-in-python\n",
    "nn_cosine = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\n",
    "nn_cosine.fit(tfidf_weight)\n",
    "obama_index = wiki[wiki['name'] == 'Barack Obama'].index[0]\n",
    "cosine, indices = nn_cosine.kneighbors(tfidf_weight[obama_index], n_neighbors = 100)\n",
    "\n",
    "# 1. flatten the 2d-array distance and indices to 1d\n",
    "# 2. merge the distance information with the original wiki dataset, \n",
    "#    to obtain the name matching the id.\n",
    "neighbors_cosine = pd.DataFrame({'cosine': cosine.flatten(), 'id': indices.flatten()})\n",
    "nearest_info = (wiki.\n",
    "                merge(neighbors_cosine, right_on = 'id', left_index = True).\n",
    "                sort_values('cosine')[['id', 'name', 'cosine']])\n",
    "\n",
    "nearest_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki['tfidf_weight'] = unpack_word_weight(tfidf_vect, tfidf_weight)\n",
    "wiki.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_obama = get_top_words(wiki, name = 'Barack Obama', column_name = 'tfidf_weight')\n",
    "words_biden = get_top_words(wiki, name = 'Joe Biden', column_name = 'tfidf_weight')\n",
    "\n",
    "words_combined = (words_obama.\n",
    "                  merge(words_biden, on = 'word').\n",
    "                  rename(columns = {'weight_x': 'Obama', 'weight_y': 'Biden'}))\n",
    "words_combined.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll extract the 5 most frequent\n",
    "common_words = set(words_combined['word'].head(5))\n",
    "print('top 5 common words: ', common_words)\n",
    "\n",
    "wiki['has_top_words'] = wiki['tfidf_weight'].apply(has_top_words, args = (common_words,))\n",
    "print('number of articles that also contain the common words: ', wiki['has_top_words'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the huge difference in this calculation using TF-IDF scores instead  of raw word counts. We've eliminated noise arising from extremely common words. Happily ever after? Not so fast. To illustrate another possible issue, let's first compute the length of each Wikipedia document, and examine the document lengths for the 100 nearest neighbors to Obama's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(row):\n",
    "    return len(row.split(' '))\n",
    "\n",
    "\n",
    "wiki['length'] = wiki['text'].apply(compute_length) \n",
    "wiki.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_cosine = (wiki.\n",
    "                  merge(neighbors_cosine, right_on = 'id', left_index = True).\n",
    "                  sort_values('cosine')[['id', 'name', 'cosine', 'length']])\n",
    "\n",
    "nearest_cosine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10.5, 4.5))\n",
    "plt.hist(wiki['length'], 50, histtype = 'stepfilled', \n",
    "         normed = True, label = 'Entire Wikipedia', alpha = 0.5)\n",
    "\n",
    "plt.hist(nearest_cosine['length'], 50, histtype = 'stepfilled', \n",
    "         normed = True, label = '100 NNs of Obama (cosine)', alpha = 0.8)\n",
    "\n",
    "plt.axvline(nearest_cosine.loc[nearest_cosine['name'] == 'Barack Obama', 'length'].values, \n",
    "            color = 'g', linestyle = '--', linewidth = 4,\n",
    "            label = 'Length of Barack Obama')\n",
    "\n",
    "plt.axis([0, 1500, 0, 0.04])\n",
    "plt.legend(loc = 'best', prop = {'size': 15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is basically telling us that 100 nearest neighbors using TF-IDF weighting and cosine distance provide a sampling across articles with different length. The thing is that whether we're choosing euclidean or cosine distance to measure our articles' similarity, both of them still ignore the document's length, which may be great in certain situations but not in others. For instance, consider the following (admittedly contrived) tweet.\n",
    "\n",
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                                             +--------+ |\n",
    "|  One that shall not be named                | Follow | |\n",
    "|  @username                                  +--------+ |\n",
    "|                                                        |\n",
    "|  Democratic governments control law in response to     |\n",
    "|  popular act.                                          |\n",
    "|                                                        |\n",
    "|  8:05 AM - 16 May 2016                                 |\n",
    "|                                                        |\n",
    "|  Reply   Retweet (1,332)   Like (300)                  |\n",
    "|                                                        |\n",
    "+--------------------------------------------------------+\n",
    "```\n",
    "\n",
    "How similar is this tweet to Barack Obama's Wikipedia article? Let's transform the tweet into TF-IDF features; compute the cosine distance between the Barack Obama article and this tweet and compare this distance to the distance between the Barack Obama article and all of its Wikipedia 10 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tfidf_vect.transform(['democratic governments control law in response to popular act'])\n",
    "tweet_dist = paired_distances(tfidf_weight[obama_index], tweet, metric = 'cosine')\n",
    "\n",
    "# compare to the 100 articles that were nearest to Obama's,\n",
    "# the distance of this tweet is shorter than how many articles\n",
    "np.sum(tweet_dist < nearest_cosine['cosine'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine distances, the tweet is \"nearer\" to Barack Obama than most of the articles. If someone is reading the Barack Obama Wikipedia page, would we want to recommend they read this tweet? Ignoring article lengths completely resulted in nonsensical results. In practice, it is common to enforce maximum or minimum document lengths. After all, when someone is reading a long article from *The Atlantic*, we wouldn't recommend him/her a tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Coursera: Washington Clustering & Retrieval](https://www.coursera.org/learn/ml-clustering-and-retrieval)\n",
    "- [Notebooks: Tf-idf Walkthrough for scikit-learn](http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/tfidf_scikit-learn.ipynb)\n",
    "- [Blog: Machine Learning :: Text feature extraction (tf-idf)](http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "126px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
